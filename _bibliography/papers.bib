---
---

@inproceedings{masry-etal-2023-unichart,
    title = "{U}ni{C}hart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning",
    author = "Masry, Ahmed  and
      Kavehzadeh, Parsa  and
      Do, Xuan Long  and
      Hoque, Enamul  and
      Joty, Shafiq",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.906",
    doi = "10.18653/v1/2023.emnlp-main.906",
    pages = "14662--14684",
    selected={true},
    abstract = "Charts are widely used for data analysis, providing visual representations and insights into complex data. To facilitate chart-based data analysis using natural language, several downstream tasks have been introduced recently such as chart question answering and chart summarization. However, existing methods for these tasks often rely on pretraining on language or vision-language tasks, neglecting the explicit modeling of chart structures (e.g., how chart elements are related to each other). To address this, we first build a large corpus of charts covering diverse topics and visual styles. We then present UniChart, a pretrained model for chart comprehension and reasoning. UniChart encodes the relevant text, data, and visual elements of charts and then uses a chart-grounded text decoder for text generation. We propose several chart-specific pretraining tasks that include: (i) low-level tasks to extract the visual elements (e.g., bars, lines) and data from charts, and (ii) high-level tasks to acquire chart understanding and reasoning skills. Our experiments demonstrate that pretraining UniChart on a large corpus with chart-specific objectives, followed by fine-tuning, yields state-of-the-art performance on four downstream tasks. Moreover, our model exhibits superior generalizability to unseen chart corpus, surpassing previous approaches that lack chart-specific objectives and utilize limited chart resources.",
}

@misc{kavehzadeh2024s2dsortedspeculativedecoding,
      title={S2D: Sorted Speculative Decoding For More Efficient Deployment of Nested Large Language Models}, 
      author={Parsa Kavehzadeh and Mohammadreza Pourreza and Mojtaba Valipour and Tinashu Zhu and Haoli Bai and Ali Ghodsi and Boxing Chen and Mehdi Rezagholizadeh},
      year={2024},
      eprint={2407.01955},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      selected={true},
      url={https://arxiv.org/abs/2407.01955}, 
}

@inproceedings{kavehzadeh-etal-2024-sorted,
    title = "Sorted {LL}a{MA}: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference",
    author = "Kavehzadeh, Parsa  and
      Valipour, Mojtaba  and
      Tahaei, Marzieh  and
      Ghodsi, Ali  and
      Chen, Boxing  and
      Rezagholizadeh, Mehdi",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2024",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-eacl.141",
    pages = "2129--2145",
    selected={true},
    abstract = "Large language models (LLMs) have revolutionized natural language processing (NLP) by excelling at understanding and generating human-like text. However, their widespread deployment can be prohibitively expensive. SortedNet is a recent training technique for enabling dynamic inference by leveraging the modularity in networks and sorting sub-models based on computation/accuracy in a nested manner. We extend SortedNet to generative NLP tasks, making large language models dynamic without any Pre-Training and by only replacing Standard Fine-Tuning (SFT) with Sorted Fine-Tuning (SoFT). Our approach boosts model efficiency, eliminating the need for multiple models for various scenarios during inference. We show that this approach can unlock the potential of intermediate layers of transformers in generating the target output. Our sub-models remain integral components of the original model, minimizing storage requirements and transition costs between different computational/latency budgets. The efficacy of our proposed method was demonstrated by applying it to tune LLaMA 2 13B on the Stanford Alpaca dataset for instruction following and TriviaQA for closed-book question answering. Our results show the superior performance of sub-models in comparison to Standard Fine-Tuning and SFT+ICT (Early-Exit), all achieved with very efficient tuning and without additional memory usage during inference.",
}


@misc{do2023llmsworkchartsdesigning,
      title={Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question Answering and Summarization}, 
      author={Xuan Long Do and Mohammad Hassanpour and Ahmed Masry and Parsa Kavehzadeh and Enamul Hoque and Shafiq Joty},
      year={2023},
      eprint={2312.10610},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.10610}, 
}

@misc{valipour2024sortednetscalablegeneralizedframework,
      title={SortedNet: A Scalable and Generalized Framework for Training Modular Deep Neural Networks}, 
      author={Mojtaba Valipour and Mehdi Rezagholizadeh and Hossein Rajabzadeh and Parsa Kavehzadeh and Marzieh Tahaei and Boxing Chen and Ali Ghodsi},
      year={2024},
      eprint={2309.00255},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.00255}, 
}

@misc{hoque2022chartquestionansweringstate,
      title={Chart Question Answering: State of the Art and Future Directions}, 
      author={Enamul Hoque and Parsa Kavehzadeh and Ahmed Masry},
      year={2022},
      eprint={2205.03966},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.03966}, 
}

@inproceedings{10.1145/3459955.3460597,
author = {Kavehzadeh, Parsa and Samadi, Mohammadreza and Amir Haeri, Maryam},
title = {Unsupervised Anomaly Detection on Node Attributed Networks: A Deep Learning Approach},
year = {2021},
isbn = {9781450389136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459955.3460597},
doi = {10.1145/3459955.3460597},
abstract = {Anomaly detection has been one of the important issues in social network analysis in recent years due to the crucial role it plays in different applications such as fraud and spammer detection. Using both graph and node characteristics leads to more accurate results in detecting anomalous nodes of node attributed networks. Most of the research works in this field are concentrated on supervised methods for anomaly detection. However, in real-world problems, there is not enough labeled data to use supervised methods for anomaly detection. This paper proposes an unsupervised method for detecting anomalous nodes in node attributed networks. The methods used a two-step deep learning approach. In the first step, structural features of the network are extracted using node2vec; in the next step, Variational AutoEncoder (VAE) is used to detect the anomalies considering both structural and node attributes. The anomalous nodes are recognized by their higher reconstruction loss. Our experimental results on two datasets, BlogCatalog and Flickr, show that the suggested method can compete with the state-of-the-art approaches of anomaly detection in attributed networks. Our method (Deep2NAD) outperforms the state-of-the-art result on the Flickr dataset based on AUC. Moreover, it receives an acceptable AUC over the BlogCatalog dataset in comparison to the state-of-the-art methods.},
booktitle = {Proceedings of the 4th International Conference on Information Science and Systems},
pages = {35â€“40},
numpages = {6},
keywords = {Anomaly Detection, Attributed Networks, Deep Learning, Unsupervised Learning},
location = {Edinburgh, United Kingdom},
series = {ICISS '21}
}


@article {
author = {Kavehzadeh, P. and Abdollah Pour, M. M. and Momtazi, S.},
title = {A Transformer-based Approach for Persian Text Chunking},
journal = {Journal of AI and Data Mining},
volume = {10},
number = {3},
pages = {373-383},
year  = {2022},
publisher = {Shahrood University of Technology},
issn = {2322-5211}, 
eissn = {2322-4444}, 
doi = {10.22044/jadm.2022.11035.2250},
abstract = {Over the last few years, text chunking has taken a significant part in sequence labeling tasks. Although a large variety of methods have been proposed for shallow parsing in English, most proposed approaches for text chunking in Persian language are based on simple and traditional concepts. In this paper, we propose using the state-of-the-art transformer-based contextualized models, namely BERT and XLM-RoBERTa, as the major structure of our models. Conditional Random Field (CRF), the combination of Bidirectional Long Short-Term Memory (BiLSTM) and CRF, and a simple dense layer are employed after the transformer-based models to enhance the model's performance in predicting chunk labels. Moreover, we provide a new dataset for noun phrase chunking in Persian which includes annotated data of Persian news text. Our experiments reveal that XLM-RoBERTa achieves the best performance between all the architectures tried on the proposed dataset. The results also show that using a single CRF layer would yield better results than a dense layer and even the combination of BiLSTM and CRF.},
keywords = {Persian text chunking,sequence labeling,deep learning,contextualized word representation},	
url = {https://jad.shahroodut.ac.ir/article_2455.html},
eprint = {https://jad.shahroodut.ac.ir/article_2455_bb18bb2f7d37ea3cb28427dd014d7074.pdf}
}

@article{
author = {Parsa KavehzadehandMohammad Mahdi  Abdollah PourandSaeedeh Momtazi},
title = {Deep Transformer-based Representation for Text Chunking},
journal = {Journal of Information Systems and Telecommunication (JIST) },
volume = {11},
number = {3},
page = {176-184},
year = {2023},
publisher = {Iranian Academic Center for Education,Culture and Research },
issn = {2322-1437},
eissn = {2345-2773},
doi = {10.61186/jist.19894.11.43.176},
abstract = {Text chunking is one of the basic tasks in natural language processing. Most proposed models in recent years were employed on chunking and other sequence labeling tasks simultaneously and they were mostly based on Recurrent Neural Networks (RNN) and Conditional Random Field (CRF). In this article, we use state-of-the-art transformer-based models in combination with CRF, Long Short-Term Memory (LSTM)-CRF as well as a simple dense layer to study the impact of different pre-trained models on the overall performance in text chunking. To this aim, we evaluate BERT, RoBERTa, Funnel Transformer, XLM, XLM-RoBERTa, BART, and GPT2 as candidates of contextualized models. Our experiments exhibit that all transformer-based models except GPT2 achieved close and high scores on text chunking. Due to the unique unidirectional architecture of GPT2, it shows a relatively poor performance on text chunking in comparison to other bidirectional transformer-based architectures. Our experiments also revealed that adding a LSTM layer to transformer-based models does not significantly improve the results since LSTM does not add additional features to assist the model to achieve more information from the input compared to the deep contextualized models.},
keywords = {Text Chunking, Sequence labeling, Contextualized Word Representation, Deep learning, Transformers},
title_fa = {Deep Transformer-based Representation for Text Chunking},
abstract_fa = {Text chunking is one of the basic tasks in natural language processing. Most proposed models in recent years were employed on chunking and other sequence labeling tasks simultaneously and they were mostly based on Recurrent Neural Networks (RNN) and Conditional Random Field (CRF). In this article, we use state-of-the-art transformer-based models in combination with CRF, Long Short-Term Memory (LSTM)-CRF as well as a simple dense layer to study the impact of different pre-trained models on the overall performance in text chunking. To this aim, we evaluate BERT, RoBERTa, Funnel Transformer, XLM, XLM-RoBERTa, BART, and GPT2 as candidates of contextualized models. Our experiments exhibit that all transformer-based models except GPT2 achieved close and high scores on text chunking. Due to the unique unidirectional architecture of GPT2, it shows a relatively poor performance on text chunking in comparison to other bidirectional transformer-based architectures. Our experiments also revealed that adding a LSTM layer to transformer-based models does not significantly improve the results since LSTM does not add additional features to assist the model to achieve more information from the input compared to the deep contextualized models.},
keywords_fa = {Text Chunking, Sequence labeling, Contextualized Word Representation, Deep learning, Transformers},
URL = {rimag.ir/fa/Article/19894},
eprint = {rimag.ir/fa/Article/Download/19894},

